{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"final_trainer.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Ts3VIlxeTMjo"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-3v9F-oOUa-3"},"source":["import os\n","import numpy as np\n","from datetime import datetime"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R9IGwDl0UeFZ"},"source":["# Root folder. Change this according to your folder structure.\n","ROOT_DIR = '/deeplearning/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RTEn4GeTUiJA"},"source":["zipped_data = os.path.join(ROOT_DIR, 'cse7512-00-dl-tp-pt1.zip')\n","\n","data_dir = '/content/data'\n","os.makedirs(data_dir, exist_ok=True)\n","!cp '{zipped_data}' '{data_dir}'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZGU7sFsqUzTx"},"source":["!unzip -qq '{data_dir}/cse7512-00-dl-tp-pt1.zip' -d '{data_dir}'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lvWo5-QMU1l-"},"source":["train_dir = os.path.join(data_dir, 'train')\n","test_dir = os.path.join(data_dir, 'test')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pEPrRihhWaJ8"},"source":["# Model\n"," Define your model here."]},{"cell_type":"markdown","metadata":{"id":"_MXxL0dcWiGp"},"source":["## ResNet"]},{"cell_type":"code","metadata":{"id":"CkWXix_-Qp6m"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","        # self.activation = nn.ReLU()\n","        self.activation = nn.ELU()\n","\n","    def forward(self, x):\n","        out = self.activation(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = self.activation(out)\n","        return out\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 96, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 126, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 160, num_blocks[3], stride=2)\n","\n","        self.linear1 = nn.Linear(1440*block.expansion, 64)\n","        self.classifier = nn.Linear(64, num_classes)\n","\n","        # self.activation = nn.ReLU()\n","        self.activation = nn.ELU()\n","        \n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","\n","        out = self.activation(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","\n","        # out = out.view(out.size(0), -1)\n","        out = out.reshape(out.size(0), out.size(1)*out.size(2)*out.size(3))\n","        \n","        out = self.activation(self.linear1(out))\n","        out = self.classifier(out)\n","        \n","        return out\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JMVbqmrCW6iW"},"source":["# Utils"]},{"cell_type":"code","metadata":{"id":"1RDY5GdyWgHL"},"source":["# Get your model here. This function returns the actual model and the name. The name is not important and can be an empty string.\n","def Model():\n","    return ResNet(BasicBlock, [2, 2, 2, 2]), 'ResNet'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"55HB2ubGYPx0"},"source":["# Some notes for logging purposes.\n","note = 'ResNet_v6. Random affine, perspective, and erasing are included. Random erasing area increased abit.'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rS-p6DAzXr32"},"source":["# Check"]},{"cell_type":"code","metadata":{"id":"xdrO8g145jS8"},"source":["# Shape check. Check if the model is outputing proper shapes.\n","model, model_architecture = Model()\n","y = model(torch.randn(1, 3, 96, 96))\n","print(y.size())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fRNfvUgcXvLL"},"source":["# Check number of parameters of your model.\n","model, model_architecture = Model()\n","pytorch_total_params = sum(p.numel() for p in model.parameters())\n","\n","print(f\"Number of parameters: {pytorch_total_params}\")\n","\n","print(int(pytorch_total_params))\n","print(2000000)\n","\n","assert int(pytorch_total_params) <= 2000000, 'Your model has the number of parameters more than 2 millions..'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9yccG4U4XHdg"},"source":["class AverageMeter(object):\n","    r\"\"\"Computes and stores the average and current value\n","    \"\"\"\n","    def __init__(self, name, fmt=':f'):\n","        self.name = name\n","        self.fmt = fmt\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        \n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","    def __str__(self):\n","        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n","        return fmtstr.format(**self.__dict__)\n","\n","\n","class ProgressMeter(object):\n","    def __init__(self, num_batches, *meters, prefix=\"\"):\n","        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n","        self.meters = meters\n","        self.prefix = prefix\n","\n","    def print(self, batch):\n","        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n","        entries += [str(meter) for meter in self.meters]\n","        print('\\t'.join(entries))\n","\n","    def _get_batch_fmtstr(self, num_batches):\n","        num_digits = len(str(num_batches // 1))\n","        fmt = '{:' + str(num_digits) + 'd}'\n","        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n","\n","\n","def accuracy(output, target, topk=(1,)):\n","    r\"\"\"Computes the accuracy over the $k$ top predictions for the specified values of k\n","    \"\"\"\n","    with torch.no_grad():\n","        maxk = max(topk)\n","        batch_size = target.size(0)\n","\n","        # _, pred = output.topk(maxk, 1, True, True)\n","        # pred = pred.t()\n","        # correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","        # faster topk (ref: https://github.com/pytorch/pytorch/issues/22812)\n","        _, idx = output.sort(descending=True)\n","        pred = idx[:,:maxk]\n","        pred = pred.t()\n","        correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","        res = []\n","        for k in topk:\n","            # correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n","            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n","            res.append(correct_k.mul_(100.0 / batch_size))\n","        return res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o-RRJHIiXK11"},"source":["# Util parameters.\n","# current_time = '20201115035927'                               # Use this one if you want to continue from last checkpoint.\n","current_time = datetime.now().strftime('%Y%m%d%H%M%S')          # Use this one when for training from scratch.\n","SAVEPATH = os.path.join(ROOT_DIR, 'checkpoints', str(current_time) + '_full_data') # Save path for model, logs and note.\n","LOG_DIR = os.path.join(SAVEPATH, 'logs')\n","os.makedirs(SAVEPATH, exist_ok=True)\n","\n","PRINTFREQ = 10"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PHsSYXQeYD8N"},"source":["# Train Model"]},{"cell_type":"code","metadata":{"id":"dedABjzqYFRF"},"source":["import time\n","\n","import torch\n","import torch.nn as nn\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NlF4fH_4YIck"},"source":["def train(train_loader, epoch, model, optimizer, criterion):\n","    batch_time = AverageMeter('Time', ':6.3f')\n","    data_time = AverageMeter('Data', ':6.3f')\n","    losses = AverageMeter('Loss', ':.4e')\n","    top1 = AverageMeter('Acc@1', ':6.2f')\n","    top5 = AverageMeter('Acc@5', ':6.2f')\n","    progress = ProgressMeter(len(train_loader), batch_time, data_time, losses,\n","                             top1, top5, prefix=\"Epoch: [{}]\".format(epoch))\n","    # switch to train mode\n","    model.train()\n","\n","    end = time.time()\n","    for i, (input, target) in enumerate(train_loader):\n","        # measure data loading time\n","        data_time.update(time.time() - end)\n","\n","        input = input.cuda()\n","        target = target.cuda()\n","\n","        # compute output\n","        output = model(input)\n","        loss = criterion(output, target)\n","\n","        # measure accuracy and record loss, accuracy \n","        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n","        losses.update(loss.item(), input.size(0))\n","        top1.update(acc1[0].item(), input.size(0))\n","        top5.update(acc5[0].item(), input.size(0))\n","\n","        # compute gradient and do SGD step\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","\n","        if i % PRINTFREQ == 0:\n","            progress.print(i)\n","\n","    print('Training => Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n","    return top1.avg, top5.avg, losses.avg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EnRrK3j9YKP1"},"source":["# Load tensorboard. \n","%load_ext tensorboard\n","%tensorboard --logdir '{LOG_DIR}'\n","\n","# After a few minutes into the training, reload the below tensorboard to see your progress."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JnHDa1URXRPW"},"source":["# Hyperparameters\n"]},{"cell_type":"code","metadata":{"id":"uhoWoiNrXQY4"},"source":["BATCHSIZE = 64\n","INITIAL_EPOCH = 0\n","# INITIAL_EPOCH = 2299 + 1  # For training from a checkpoint.\n","EPOCHS = 1840\n","LR = 1e-4"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qQffFFCJFm3C"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"BBjMEtNw7nIf"},"source":["model, model_architecture = Model()\n","model = model.cuda()\n","\n","# model_name = 'train_acc98.7000_train_loss_0.0360_epoch_2299.pth'              # Use for training from a checkpoint.\n","# model.load_state_dict(torch.load(os.path.join(SAVEPATH, model_name)))         # Load checkpoint model.\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","criterion = criterion.cuda()\n","\n","normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","\n","# Training augmentations.\n","train_transform = transforms.Compose([\n","    transforms.RandomCrop(96, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ColorJitter(brightness=(0.5, 2.5), contrast=(0.5, 2.5), saturation=(0.5, 2.5), hue=(-0.1, 0.1)),\n","    transforms.ToTensor(),\n","    transforms.RandomErasing(p=0.5, scale=(0.2, 0.2), ratio=(1, 1)),\n","    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), shear=5),\n","    transforms.RandomPerspective(distortion_scale=0.25),\n","    normalize\n","])\n","\n","train_dataset = torchvision.datasets.ImageFolder(train_dir, transform=train_transform)\n","train_loader = DataLoader(train_dataset,\n","                            batch_size=BATCHSIZE, shuffle=True,\n","                            num_workers=0, pin_memory=True)\n","\n","# Parameters for logging.\n","writer = SummaryWriter(LOG_DIR)\n","\n","top1_acc_train = None\n","top5_acc_train = None\n","loss_train = None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a1dp2qChYMCm"},"source":["for epoch in range(INITIAL_EPOCH, EPOCHS):\n","\n","    print(\"\\n----- epoch: {}, lr: {} -----\".format(\n","        epoch, optimizer.param_groups[0][\"lr\"]))\n","    \n","    start_time = time.time()\n","\n","    # Train\n","    top1_acc_train, top5_acc_train, loss_train = train(train_loader, epoch, model, optimizer, criterion)\n","    writer.add_scalar('accuracy/top1_train', top1_acc_train, epoch)\n","    writer.add_scalar('accuracy/top5_train', top5_acc_train, epoch)\n","    writer.add_scalar('loss/train', loss_train, epoch)\n","\n","    elapsed_time = time.time() - start_time\n","    print('==> {:.2f} seconds to train this epoch\\n'.format(elapsed_time))\n","\n","    model_name = 'train_acc{:.4f}_train_loss_{:.4f}_epoch_{}.pth'.format(top1_acc_train, loss_train, epoch)\n","    torch.save(model.state_dict(), os.path.join(SAVEPATH, model_name))\n","\n","    # learning rate scheduling\n","    # scheduler.step()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BGfFViaNtBNK"},"source":["print(f\"Train Top-1 Accuracy: {top1_acc_train}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YJfGHxW-mli3"},"source":["def save_model_info():\n","    info_file = os.path.join(SAVEPATH, 'info.txt')\n","    info = '''\n","    MODEL : {}\n","    EPOCHS : {}\\n \n","    optimizer : {}\\n\n","    criterion : {}\\n\n","    train_transform : {}\\n\\n\n","\n","    train_acc_top1 : {}\\n\n","    loss_train : {}\\n\n","\n","    pytorch_total_params : {}\\n\\n\n","    \n","    note : {}\\n\n","    '''.format(model_architecture, EPOCHS, optimizer, criterion, train_transform, top1_acc_train, loss_train, pytorch_total_params, note)\n","    \n","    with open(info_file, 'w') as f:\n","        f.write(info)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iECbF6Gmm3l-"},"source":["save_model_info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_9jFt7xNnK17"},"source":["# Make an evalutation csv file"]},{"cell_type":"code","metadata":{"id":"V59w0GumGbtm"},"source":["epochs = EPOCHS - 1 # Just for using in the submission file name.\n","\n","# Use this to output submission file from your desire model checkpoint.\n","\n","# model_name = 'train_acc98.4600_train_loss_0.0450_epoch_2355.pth'                      \n","# SAVEPATH = os.path.join(ROOT_DIR, 'checkpoints', '20201106080213' + '_full_data')\n","# epochs = 2355"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_-SAAxVAna3m"},"source":["import torch\n","import pandas as pd\n","import argparse\n","import time\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T0fEZ8s0ndQO"},"source":["def eval():\n","    ########## You can change this part only in this cell ##########\n","    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    test_transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        normalize\n","    ])\n","    ################################################################\n","\n","    test_dataset = torchvision.datasets.ImageFolder(test_dir, transform=test_transform)\n","    test_loader = DataLoader(test_dataset, batch_size=BATCHSIZE, num_workers=0, shuffle=False)\n","\n","    model, _ = Model()\n","    model = model.cuda()\n","\n","    # I just changed the model path here.\n","    model.load_state_dict(torch.load(os.path.join(SAVEPATH, model_name)))\n","\n","    print('Make an evaluation csv file for kaggle submission...')\n","    Category = []\n","    for input, _ in test_loader:\n","        input = input.cuda()\n","        output = model(input)\n","        output = torch.argmax(output, dim=1)\n","        Category = Category + output.tolist()\n","\n","    Id = list(range(0, 8000))\n","    samples = {\n","       'Id': Id,\n","       'Category': Category \n","    }\n","    df = pd.DataFrame(samples, columns=['Id', 'Category'])\n","\n","    # I just changed the submission file name here.\n","    df.to_csv(os.path.join(SAVEPATH, 'submission_epochs_{}.csv'.format(epochs)), index=False)\n","    print('Done!!')\n","\n","\n","if __name__ == \"__main__\":\n","    eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xRJwZk_viLov"},"source":[""],"execution_count":null,"outputs":[]}]}